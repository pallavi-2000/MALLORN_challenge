{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåü MALLORN V5: Physics-Based Golden Features\n",
    "\n",
    "## Strategy: Feature Engineering > Augmentation\n",
    "\n",
    "Based on competition discussion (24th place): *\"Focus on light curve features that distinguish TDE from SN and AGN, especially the shape.\"*\n",
    "\n",
    "### TDE vs SN vs AGN - Key Physical Differences\n",
    "\n",
    "| Feature | TDE | Supernova | AGN |\n",
    "|---------|-----|-----------|-----|\n",
    "| Rise time | **Slow (weeks-months)** | Fast (~2 weeks) | Stochastic |\n",
    "| Decay | **t^(-5/3) power law** | Exponential | Random |\n",
    "| Duration | 100-300+ days | 50-100 days | Years |\n",
    "| Color | **Very blue (u-band strong)** | Varies | Redder |\n",
    "| Symmetry | **Asymmetric (decay >> rise)** | More symmetric | N/A |\n",
    "| Smoothness | **Very smooth** | Smooth | **Stochastic/flickering** |\n",
    "| Temperature | **~30,000-50,000K (hot!)** | ~10,000K | Variable |\n",
    "\n",
    "### V5 Approach\n",
    "1. **NO augmentation** (lessons learned: it hurts)\n",
    "2. Focus on **shape-based discriminative features**\n",
    "3. Add **temperature proxy** via color at peak\n",
    "4. Add **TDE-specific decay fitting**\n",
    "5. Add **AGN stochasticity detectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: Install Dependencies\n",
    "!pip install -q lightgbm xgboost extinction kaggle\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: Setup Kaggle API\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "print(\"üì§ Please upload your kaggle.json file:\")\n",
    "uploaded = files.upload()\n",
    "!mv kaggle.json /root/.kaggle/\n",
    "!chmod 600 /root/.kaggle/kaggle.json\n",
    "print(\"\\n‚úÖ Kaggle API configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: Download Data\n",
    "!mkdir -p /content/mallorn_data\n",
    "print(\"üì• Downloading MALLORN competition data...\")\n",
    "!kaggle competitions download -c mallorn-astronomical-classification-challenge -p /content/mallorn_data\n",
    "print(\"\\nüì¶ Extracting data...\")\n",
    "!cd /content/mallorn_data && unzip -q -o mallorn-astronomical-classification-challenge.zip\n",
    "print(\"\\n‚úÖ Data downloaded and extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit, minimize\n",
    "from scipy import stats\n",
    "from scipy.interpolate import interp1d\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from extinction import fitzpatrick99\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: Configuration\n",
    "DATA_PATH = '/content/mallorn_data'\n",
    "\n",
    "# LSST filter effective wavelengths (Angstroms)\n",
    "FILTER_WAVELENGTHS = {\n",
    "    'u': 3641, 'g': 4704, 'r': 6155,\n",
    "    'i': 7504, 'z': 8695, 'y': 10056\n",
    "}\n",
    "\n",
    "# Physics constants\n",
    "TDE_DECAY_INDEX = -5/3  # Theoretical t^(-5/3) fallback rate\n",
    "\n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"‚úÖ Configuration set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6: Load Data\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_log = pd.read_csv(f'{DATA_PATH}/train_log.csv')\n",
    "test_log = pd.read_csv(f'{DATA_PATH}/test_log.csv')\n",
    "\n",
    "print(f\"‚úÖ Training objects: {len(train_log)}\")\n",
    "print(f\"‚úÖ Test objects: {len(test_log)}\")\n",
    "print(f\"\\nüìä Class distribution:\")\n",
    "print(f\"   TDE: {train_log['target'].sum()} ({100*train_log['target'].mean():.2f}%)\")\n",
    "print(f\"   Non-TDE: {(train_log['target'] == 0).sum()}\")\n",
    "\n",
    "# Check SpecType distribution\n",
    "if 'SpecType' in train_log.columns:\n",
    "    print(f\"\\nüìä Spectral types:\")\n",
    "    print(train_log['SpecType'].value_counts())\n",
    "\n",
    "# Load lightcurves\n",
    "train_lightcurves = {}\n",
    "test_lightcurves = {}\n",
    "splits = [f'split_{i:02d}' for i in range(1, 21)]\n",
    "\n",
    "for split in tqdm(splits, desc=\"Loading splits\"):\n",
    "    train_path = f'{DATA_PATH}/{split}/train_full_lightcurves.csv'\n",
    "    test_path = f'{DATA_PATH}/{split}/test_full_lightcurves.csv'\n",
    "    if os.path.exists(train_path):\n",
    "        train_lightcurves[split] = pd.read_csv(train_path)\n",
    "    if os.path.exists(test_path):\n",
    "        test_lightcurves[split] = pd.read_csv(test_path)\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(train_lightcurves)} splits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: Helper Functions\n",
    "\n",
    "def deextinct_flux(flux, ebv, filter_name):\n",
    "    \"\"\"Remove Milky Way dust extinction from flux.\"\"\"\n",
    "    if pd.isna(ebv) or ebv <= 0:\n",
    "        return np.array(flux)\n",
    "    wavelength = np.array([FILTER_WAVELENGTHS[filter_name]])\n",
    "    A_lambda = fitzpatrick99(wavelength, float(ebv) * 3.1)\n",
    "    return np.array(flux) * 10**(A_lambda[0] / 2.5)\n",
    "\n",
    "def get_lightcurve(object_id, log_df, lightcurves_dict):\n",
    "    \"\"\"Get lightcurve data for an object.\"\"\"\n",
    "    obj_info = log_df[log_df['object_id'] == object_id]\n",
    "    if len(obj_info) == 0:\n",
    "        return None, None\n",
    "    split = obj_info['split'].values[0]\n",
    "    if split not in lightcurves_dict:\n",
    "        return None, obj_info.iloc[0]\n",
    "    lc = lightcurves_dict[split]\n",
    "    return lc[lc['object_id'] == object_id].copy(), obj_info.iloc[0]\n",
    "\n",
    "print(\"‚úÖ Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8: Model Functions for Fitting\n",
    "\n",
    "def bazin_function(t, A, t0, t_fall, t_rise, c):\n",
    "    \"\"\"Bazin model for supernova-like transients.\"\"\"\n",
    "    return A * np.exp(-(t - t0) / t_fall) / (1 + np.exp(-(t - t0) / t_rise)) + c\n",
    "\n",
    "def tde_decay_model(t, A, t0, alpha):\n",
    "    \"\"\"TDE power-law decay model: F(t) = A * (t - t0)^alpha\n",
    "    For TDE: alpha should be close to -5/3 ‚âà -1.67\n",
    "    \"\"\"\n",
    "    t_shifted = np.maximum(t - t0, 0.1)  # Avoid division by zero\n",
    "    return A * np.power(t_shifted, alpha)\n",
    "\n",
    "def gaussian_rise_powerlaw_decay(t, A, t_peak, sigma_rise, alpha_decay):\n",
    "    \"\"\"Combined model: Gaussian rise + power-law decay\n",
    "    This captures TDE-like behavior better than Bazin.\n",
    "    \"\"\"\n",
    "    result = np.zeros_like(t, dtype=float)\n",
    "    \n",
    "    # Rising phase (before peak)\n",
    "    rising = t <= t_peak\n",
    "    result[rising] = A * np.exp(-0.5 * ((t[rising] - t_peak) / sigma_rise)**2)\n",
    "    \n",
    "    # Decay phase (after peak)\n",
    "    decaying = t > t_peak\n",
    "    t_decay = t[decaying] - t_peak + 1  # +1 to avoid t=0\n",
    "    result[decaying] = A * np.power(t_decay, alpha_decay)\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"‚úÖ Model functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9: GOLDEN FEATURE EXTRACTION\n",
    "\n",
    "def extract_golden_features(lc_df, info):\n",
    "    \"\"\"\n",
    "    V5 GOLDEN FEATURES - Physics-based discriminators\n",
    "    \n",
    "    Key discriminators:\n",
    "    1. SHAPE: Rise time, decay rate, asymmetry (TDE vs SNe)\n",
    "    2. COLOR: u-band strength, color at peak (TDE is BLUE)\n",
    "    3. STOCHASTICITY: Smoothness metrics (TDE vs AGN)\n",
    "    4. POWER-LAW: Deviation from t^(-5/3) decay (TDE signature)\n",
    "    5. DURATION: Total evolution timescale (TDE is long)\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    if lc_df is None or len(lc_df) == 0:\n",
    "        return None\n",
    "    \n",
    "    time = lc_df['Time (MJD)'].values\n",
    "    flux = lc_df['Flux'].values\n",
    "    flux_err = lc_df['Flux_err'].values\n",
    "    bands = lc_df['Filter'].values\n",
    "    \n",
    "    z = info['Z'] if pd.notna(info['Z']) else 0\n",
    "    ebv = info['EBV'] if pd.notna(info['EBV']) else 0\n",
    "    \n",
    "    # Organize by band with de-extinction\n",
    "    lc_dict = {}\n",
    "    for filt in ['u', 'g', 'r', 'i', 'z', 'y']:\n",
    "        mask = bands == filt\n",
    "        if mask.sum() > 0:\n",
    "            f_time = time[mask]\n",
    "            f_flux = deextinct_flux(flux[mask], ebv, filt)\n",
    "            f_err = flux_err[mask]\n",
    "            # Sort by time\n",
    "            sort_idx = np.argsort(f_time)\n",
    "            lc_dict[filt] = {\n",
    "                'time': f_time[sort_idx],\n",
    "                'flux': f_flux[sort_idx],\n",
    "                'flux_err': f_err[sort_idx]\n",
    "            }\n",
    "    \n",
    "    # ==========================================\n",
    "    # SECTION 1: BASIC STATISTICS\n",
    "    # ==========================================\n",
    "    features['n_obs'] = len(flux)\n",
    "    features['n_bands'] = len(lc_dict)\n",
    "    features['duration'] = time.max() - time.min()\n",
    "    features['flux_mean'] = np.mean(flux)\n",
    "    features['flux_std'] = np.std(flux)\n",
    "    features['flux_max'] = np.max(flux)\n",
    "    features['flux_min'] = np.min(flux)\n",
    "    features['flux_range'] = features['flux_max'] - features['flux_min']\n",
    "    features['snr_median'] = np.median(flux / (flux_err + 1e-10))\n",
    "    \n",
    "    # ==========================================\n",
    "    # SECTION 2: GOLDEN FEATURE - RISE & DECAY TIMES\n",
    "    # TDE: slow rise (weeks-months), very slow decay (months-years)\n",
    "    # SNe: fast rise (~2 weeks), faster decay\n",
    "    # ==========================================\n",
    "    for filt in ['g', 'r']:\n",
    "        prefix = f'{filt}_'\n",
    "        if filt not in lc_dict or len(lc_dict[filt]['time']) < 5:\n",
    "            for key in ['rise_time', 'decay_time', 'asymmetry', 'rise_rate', \n",
    "                       'decay_rate', 'peak_flux', 'fwhm', 't10_t90']:\n",
    "                features[f'{prefix}{key}'] = np.nan\n",
    "            continue\n",
    "        \n",
    "        f_time = lc_dict[filt]['time']\n",
    "        f_flux = lc_dict[filt]['flux']\n",
    "        \n",
    "        peak_idx = np.argmax(f_flux)\n",
    "        peak_flux = f_flux[peak_idx]\n",
    "        peak_time = f_time[peak_idx]\n",
    "        \n",
    "        features[f'{prefix}peak_flux'] = peak_flux\n",
    "        \n",
    "        # Rise time (first obs to peak)\n",
    "        rise_time = peak_time - f_time[0]\n",
    "        features[f'{prefix}rise_time'] = rise_time\n",
    "        \n",
    "        # Decay time (peak to last obs)\n",
    "        decay_time = f_time[-1] - peak_time\n",
    "        features[f'{prefix}decay_time'] = decay_time\n",
    "        \n",
    "        # Asymmetry: decay/rise ratio (TDE >> 1, SNe ~ 1-3)\n",
    "        if rise_time > 1:\n",
    "            features[f'{prefix}asymmetry'] = decay_time / rise_time\n",
    "        else:\n",
    "            features[f'{prefix}asymmetry'] = np.nan\n",
    "        \n",
    "        # Rise rate (flux change per day during rise)\n",
    "        if rise_time > 1 and peak_idx > 0:\n",
    "            rise_amount = peak_flux - f_flux[0]\n",
    "            features[f'{prefix}rise_rate'] = rise_amount / rise_time\n",
    "        else:\n",
    "            features[f'{prefix}rise_rate'] = np.nan\n",
    "        \n",
    "        # Decay rate (flux change per day during decay)\n",
    "        if decay_time > 1 and peak_idx < len(f_flux) - 1:\n",
    "            decay_amount = peak_flux - f_flux[-1]\n",
    "            features[f'{prefix}decay_rate'] = decay_amount / decay_time\n",
    "        else:\n",
    "            features[f'{prefix}decay_rate'] = np.nan\n",
    "        \n",
    "        # FWHM (Full Width at Half Maximum)\n",
    "        half_max = peak_flux / 2\n",
    "        above_half = f_time[f_flux > half_max]\n",
    "        if len(above_half) >= 2:\n",
    "            features[f'{prefix}fwhm'] = above_half.max() - above_half.min()\n",
    "        else:\n",
    "            features[f'{prefix}fwhm'] = 0\n",
    "        \n",
    "        # t10-t90: time between 10% and 90% of peak\n",
    "        # This captures the overall evolution timescale\n",
    "        above_10 = f_time[f_flux > 0.1 * peak_flux]\n",
    "        above_90 = f_time[f_flux > 0.9 * peak_flux]\n",
    "        if len(above_10) >= 2:\n",
    "            features[f'{prefix}t10_t90'] = above_10.max() - above_10.min()\n",
    "        else:\n",
    "            features[f'{prefix}t10_t90'] = 0\n",
    "    \n",
    "    # ==========================================\n",
    "    # SECTION 3: GOLDEN FEATURE - POWER-LAW DECAY INDEX\n",
    "    # TDE decay ~ t^(-5/3) = t^(-1.67)\n",
    "    # ==========================================\n",
    "    for filt in ['g', 'r']:\n",
    "        prefix = f'{filt}_'\n",
    "        if filt not in lc_dict or len(lc_dict[filt]['time']) < 10:\n",
    "            features[f'{prefix}decay_power_index'] = np.nan\n",
    "            features[f'{prefix}tde_index_match'] = np.nan\n",
    "            features[f'{prefix}decay_power_r2'] = np.nan\n",
    "            continue\n",
    "        \n",
    "        f_time = lc_dict[filt]['time']\n",
    "        f_flux = lc_dict[filt]['flux']\n",
    "        \n",
    "        peak_idx = np.argmax(f_flux)\n",
    "        \n",
    "        # Get decay phase data (after peak)\n",
    "        decay_mask = np.arange(len(f_time)) > peak_idx\n",
    "        if decay_mask.sum() >= 5:\n",
    "            t_decay = f_time[decay_mask] - f_time[peak_idx] + 1  # Shift to avoid log(0)\n",
    "            f_decay = f_flux[decay_mask]\n",
    "            \n",
    "            # Only use positive flux values for log fitting\n",
    "            pos_mask = f_decay > 0\n",
    "            if pos_mask.sum() >= 5:\n",
    "                try:\n",
    "                    # Fit log(F) = log(A) + alpha * log(t)\n",
    "                    log_t = np.log(t_decay[pos_mask])\n",
    "                    log_f = np.log(f_decay[pos_mask])\n",
    "                    \n",
    "                    # Linear regression\n",
    "                    slope, intercept, r_value, _, _ = stats.linregress(log_t, log_f)\n",
    "                    \n",
    "                    features[f'{prefix}decay_power_index'] = slope\n",
    "                    features[f'{prefix}decay_power_r2'] = r_value**2\n",
    "                    \n",
    "                    # How close to TDE theoretical index?\n",
    "                    # Lower = more TDE-like\n",
    "                    features[f'{prefix}tde_index_match'] = abs(slope - TDE_DECAY_INDEX)\n",
    "                except:\n",
    "                    features[f'{prefix}decay_power_index'] = np.nan\n",
    "                    features[f'{prefix}tde_index_match'] = np.nan\n",
    "                    features[f'{prefix}decay_power_r2'] = np.nan\n",
    "            else:\n",
    "                features[f'{prefix}decay_power_index'] = np.nan\n",
    "                features[f'{prefix}tde_index_match'] = np.nan\n",
    "                features[f'{prefix}decay_power_r2'] = np.nan\n",
    "        else:\n",
    "            features[f'{prefix}decay_power_index'] = np.nan\n",
    "            features[f'{prefix}tde_index_match'] = np.nan\n",
    "            features[f'{prefix}decay_power_r2'] = np.nan\n",
    "    \n",
    "    # ==========================================\n",
    "    # SECTION 4: GOLDEN FEATURE - COLOR (TDE IS BLUE!)\n",
    "    # TDE: Strong u-band, blue colors\n",
    "    # AGN: Redder\n",
    "    # ==========================================\n",
    "    \n",
    "    # u-band strength relative to r-band (TDE signature!)\n",
    "    if 'u' in lc_dict and 'r' in lc_dict:\n",
    "        u_max = np.max(lc_dict['u']['flux'])\n",
    "        r_max = np.max(lc_dict['r']['flux'])\n",
    "        features['u_r_max_ratio'] = u_max / (r_max + 1e-10)\n",
    "        \n",
    "        u_mean = np.mean(lc_dict['u']['flux'])\n",
    "        r_mean = np.mean(lc_dict['r']['flux'])\n",
    "        features['u_r_mean_ratio'] = u_mean / (r_mean + 1e-10)\n",
    "    else:\n",
    "        features['u_r_max_ratio'] = np.nan\n",
    "        features['u_r_mean_ratio'] = np.nan\n",
    "    \n",
    "    # g-r color (proxy for temperature - TDE is hotter = bluer)\n",
    "    if 'g' in lc_dict and 'r' in lc_dict:\n",
    "        g_flux = lc_dict['g']['flux']\n",
    "        r_flux = lc_dict['r']['flux']\n",
    "        g_time = lc_dict['g']['time']\n",
    "        r_time = lc_dict['r']['time']\n",
    "        \n",
    "        # g/r ratio (higher = bluer = hotter)\n",
    "        features['g_r_max_ratio'] = np.max(g_flux) / (np.max(r_flux) + 1e-10)\n",
    "        features['g_r_mean_ratio'] = np.mean(g_flux) / (np.mean(r_flux) + 1e-10)\n",
    "        \n",
    "        # Color at peak (important discriminator!)\n",
    "        # Find g-r color near peak brightness\n",
    "        r_peak_idx = np.argmax(r_flux)\n",
    "        r_peak_time = r_time[r_peak_idx]\n",
    "        \n",
    "        # Find g flux near peak time\n",
    "        g_near_peak = np.abs(g_time - r_peak_time) < 10\n",
    "        if g_near_peak.sum() > 0:\n",
    "            g_at_peak = np.mean(g_flux[g_near_peak])\n",
    "            r_at_peak = r_flux[r_peak_idx]\n",
    "            features['g_r_at_peak'] = g_at_peak / (r_at_peak + 1e-10)\n",
    "        else:\n",
    "            features['g_r_at_peak'] = np.nan\n",
    "        \n",
    "        # Color evolution (TDE stays blue longer)\n",
    "        # Sample color at different phases\n",
    "        for phase in [30, 60, 100]:\n",
    "            target_time = r_peak_time + phase\n",
    "            g_near = np.abs(g_time - target_time) < 15\n",
    "            r_near = np.abs(r_time - target_time) < 15\n",
    "            \n",
    "            if g_near.sum() > 0 and r_near.sum() > 0:\n",
    "                features[f'g_r_color_{phase}d'] = np.mean(g_flux[g_near]) / (np.mean(r_flux[r_near]) + 1e-10)\n",
    "            else:\n",
    "                features[f'g_r_color_{phase}d'] = np.nan\n",
    "    else:\n",
    "        for key in ['g_r_max_ratio', 'g_r_mean_ratio', 'g_r_at_peak',\n",
    "                   'g_r_color_30d', 'g_r_color_60d', 'g_r_color_100d']:\n",
    "            features[key] = np.nan\n",
    "    \n",
    "    # ==========================================\n",
    "    # SECTION 5: GOLDEN FEATURE - STOCHASTICITY (AGN DETECTOR)\n",
    "    # TDE: Smooth single flare\n",
    "    # AGN: Stochastic, flickering, multiple peaks\n",
    "    # ==========================================\n",
    "    for filt in ['g', 'r']:\n",
    "        prefix = f'{filt}_'\n",
    "        if filt not in lc_dict or len(lc_dict[filt]['flux']) < 10:\n",
    "            for key in ['flux_scatter', 'chi2_smooth', 'n_local_peaks', \n",
    "                       'largest_dip_frac', 'flux_ratio_std']:\n",
    "                features[f'{prefix}{key}'] = np.nan\n",
    "            continue\n",
    "        \n",
    "        f_time = lc_dict[filt]['time']\n",
    "        f_flux = lc_dict[filt]['flux']\n",
    "        f_err = lc_dict[filt]['flux_err']\n",
    "        \n",
    "        # Excess scatter beyond measurement errors\n",
    "        # AGN has more excess scatter\n",
    "        expected_var = np.mean(f_err**2)\n",
    "        actual_var = np.var(f_flux)\n",
    "        features[f'{prefix}flux_scatter'] = (actual_var - expected_var) / (np.mean(f_flux)**2 + 1e-10)\n",
    "        \n",
    "        # Chi^2 against a smooth model (running mean)\n",
    "        # Higher = more stochastic = more AGN-like\n",
    "        window = min(5, len(f_flux) // 3)\n",
    "        if window >= 2:\n",
    "            smooth = np.convolve(f_flux, np.ones(window)/window, mode='valid')\n",
    "            if len(smooth) > 0:\n",
    "                # Pad smooth to match original length\n",
    "                pad_left = (len(f_flux) - len(smooth)) // 2\n",
    "                pad_right = len(f_flux) - len(smooth) - pad_left\n",
    "                smooth_padded = np.pad(smooth, (pad_left, pad_right), mode='edge')\n",
    "                residuals = (f_flux - smooth_padded) / (f_err + 1e-10)\n",
    "                features[f'{prefix}chi2_smooth'] = np.mean(residuals**2)\n",
    "            else:\n",
    "                features[f'{prefix}chi2_smooth'] = np.nan\n",
    "        else:\n",
    "            features[f'{prefix}chi2_smooth'] = np.nan\n",
    "        \n",
    "        # Number of local peaks (AGN has many, TDE has 1)\n",
    "        diffs = np.diff(f_flux)\n",
    "        sign_changes = np.where(diffs[:-1] * diffs[1:] < 0)[0]\n",
    "        local_maxima = [i+1 for i in sign_changes if diffs[i] > 0 and diffs[i+1] < 0]\n",
    "        features[f'{prefix}n_local_peaks'] = len(local_maxima)\n",
    "        \n",
    "        # Largest dip below peak as fraction of peak\n",
    "        # TDE monotonically decays, AGN may have dips\n",
    "        peak_flux = np.max(f_flux)\n",
    "        peak_idx = np.argmax(f_flux)\n",
    "        if peak_idx < len(f_flux) - 1:\n",
    "            post_peak_min = np.min(f_flux[peak_idx:])\n",
    "            post_peak_max = np.max(f_flux[peak_idx:])\n",
    "            features[f'{prefix}largest_dip_frac'] = (post_peak_max - post_peak_min) / (peak_flux + 1e-10)\n",
    "        else:\n",
    "            features[f'{prefix}largest_dip_frac'] = 0\n",
    "        \n",
    "        # Flux ratio variability (consecutive points)\n",
    "        flux_ratios = f_flux[1:] / (f_flux[:-1] + 1e-10)\n",
    "        features[f'{prefix}flux_ratio_std'] = np.std(flux_ratios)\n",
    "    \n",
    "    # ==========================================\n",
    "    # SECTION 6: BAZIN FIT PARAMETERS\n",
    "    # ==========================================\n",
    "    for filt in ['g', 'r']:\n",
    "        prefix = f'{filt}_baz_'\n",
    "        if filt not in lc_dict or len(lc_dict[filt]['time']) < 10:\n",
    "            for key in ['amplitude', 't0', 't_fall', 't_rise', 'baseline', 'rise_fall_ratio', 'fit_chi2']:\n",
    "                features[f'{prefix}{key}'] = np.nan\n",
    "            continue\n",
    "        \n",
    "        f_time = lc_dict[filt]['time']\n",
    "        f_flux = lc_dict[filt]['flux']\n",
    "        f_err = lc_dict[filt]['flux_err']\n",
    "        \n",
    "        try:\n",
    "            t = f_time - f_time.min()\n",
    "            peak_idx = np.argmax(f_flux)\n",
    "            A_init = f_flux[peak_idx] - np.median(f_flux)\n",
    "            t0_init = t[peak_idx]\n",
    "            \n",
    "            bounds = ([0, 0, 1, 1, -1000], [10000, t.max()+100, 1000, 500, 1000])\n",
    "            popt, _ = curve_fit(\n",
    "                bazin_function, t, f_flux,\n",
    "                p0=[max(A_init, 0.1), t0_init, 100, 30, np.median(f_flux)],\n",
    "                bounds=bounds, maxfev=3000\n",
    "            )\n",
    "            \n",
    "            features[f'{prefix}amplitude'] = popt[0]\n",
    "            features[f'{prefix}t0'] = popt[1]\n",
    "            features[f'{prefix}t_fall'] = popt[2]\n",
    "            features[f'{prefix}t_rise'] = popt[3]\n",
    "            features[f'{prefix}baseline'] = popt[4]\n",
    "            features[f'{prefix}rise_fall_ratio'] = popt[3] / (popt[2] + 0.01)\n",
    "            \n",
    "            # Fit quality\n",
    "            predicted = bazin_function(t, *popt)\n",
    "            chi2 = np.sum(((f_flux - predicted) / (f_err + 1e-10))**2) / len(f_flux)\n",
    "            features[f'{prefix}fit_chi2'] = chi2\n",
    "            \n",
    "        except:\n",
    "            for key in ['amplitude', 't0', 't_fall', 't_rise', 'baseline', 'rise_fall_ratio', 'fit_chi2']:\n",
    "                features[f'{prefix}{key}'] = np.nan\n",
    "    \n",
    "    # ==========================================\n",
    "    # SECTION 7: PER-BAND BASIC FEATURES\n",
    "    # ==========================================\n",
    "    for filt in ['u', 'g', 'r', 'i', 'z', 'y']:\n",
    "        prefix = f'{filt}_'\n",
    "        if filt not in lc_dict:\n",
    "            for key in ['n_obs', 'flux_mean', 'flux_std', 'flux_max', 'amplitude']:\n",
    "                features[f'{prefix}{key}'] = np.nan\n",
    "            continue\n",
    "        \n",
    "        f_flux = lc_dict[filt]['flux']\n",
    "        features[f'{prefix}n_obs'] = len(f_flux)\n",
    "        features[f'{prefix}flux_mean'] = np.mean(f_flux)\n",
    "        features[f'{prefix}flux_std'] = np.std(f_flux)\n",
    "        features[f'{prefix}flux_max'] = np.max(f_flux)\n",
    "        features[f'{prefix}amplitude'] = np.max(f_flux) - np.min(f_flux)\n",
    "    \n",
    "    # ==========================================\n",
    "    # SECTION 8: REDSHIFT FEATURES\n",
    "    # ==========================================\n",
    "    features['redshift'] = z\n",
    "    features['ebv'] = ebv\n",
    "    \n",
    "    # Rest-frame duration (corrected for time dilation)\n",
    "    features['rest_duration'] = features['duration'] / (1 + z) if z > 0 else features['duration']\n",
    "    \n",
    "    # Luminosity distance proxy (higher z = larger distance = fainter if same intrinsic L)\n",
    "    features['z_squared'] = z**2\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"‚úÖ Golden feature extraction defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: Extract Features\n",
    "print(\"=\"*70)\n",
    "print(\"EXTRACTING GOLDEN FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Training features\n",
    "train_features = []\n",
    "for _, row in tqdm(train_log.iterrows(), total=len(train_log), desc=\"Train\"):\n",
    "    lc, info = get_lightcurve(row['object_id'], train_log, train_lightcurves)\n",
    "    feats = extract_golden_features(lc, info)\n",
    "    if feats is not None:\n",
    "        feats['object_id'] = row['object_id']\n",
    "        feats['target'] = row['target']\n",
    "        train_features.append(feats)\n",
    "\n",
    "train_df = pd.DataFrame(train_features)\n",
    "print(f\"\\n‚úÖ Extracted features for {len(train_df)} training objects\")\n",
    "print(f\"   Number of features: {len([c for c in train_df.columns if c not in ['object_id', 'target']])}\")\n",
    "\n",
    "# Test features\n",
    "test_features = []\n",
    "for _, row in tqdm(test_log.iterrows(), total=len(test_log), desc=\"Test\"):\n",
    "    lc, info = get_lightcurve(row['object_id'], test_log, test_lightcurves)\n",
    "    feats = extract_golden_features(lc, info)\n",
    "    if feats is not None:\n",
    "        feats['object_id'] = row['object_id']\n",
    "        test_features.append(feats)\n",
    "\n",
    "test_df = pd.DataFrame(test_features)\n",
    "print(f\"\\n‚úÖ Extracted features for {len(test_df)} test objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11: Analyze Feature Distributions\n",
    "print(\"=\"*70)\n",
    "print(\"FEATURE ANALYSIS: TDE vs Non-TDE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Key discriminative features to check\n",
    "key_features = [\n",
    "    'r_asymmetry', 'g_asymmetry',  # TDE should be higher\n",
    "    'r_rise_time', 'g_rise_time',  # TDE should be longer\n",
    "    'r_tde_index_match', 'g_tde_index_match',  # TDE should be lower\n",
    "    'u_r_max_ratio', 'g_r_at_peak',  # TDE should be higher (bluer)\n",
    "    'r_n_local_peaks', 'g_n_local_peaks',  # TDE should be ~1\n",
    "    'r_fwhm', 'g_fwhm',  # TDE should be wider\n",
    "]\n",
    "\n",
    "tde_mask = train_df['target'] == 1\n",
    "\n",
    "print(f\"\\n{'Feature':<25} {'TDE Mean':>12} {'Non-TDE Mean':>12} {'Diff':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for feat in key_features:\n",
    "    if feat in train_df.columns:\n",
    "        tde_mean = train_df.loc[tde_mask, feat].mean()\n",
    "        nontde_mean = train_df.loc[~tde_mask, feat].mean()\n",
    "        diff = tde_mean - nontde_mean\n",
    "        print(f\"{feat:<25} {tde_mean:>12.4f} {nontde_mean:>12.4f} {diff:>10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 12: Prepare Data\n",
    "print(\"=\"*70)\n",
    "print(\"PREPARING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "feature_cols = [c for c in train_df.columns if c not in ['object_id', 'target']]\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "\n",
    "X = train_df[feature_cols].copy()\n",
    "y = train_df['target'].values\n",
    "X_test = test_df[feature_cols].copy()\n",
    "\n",
    "# Handle missing values\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "for col in feature_cols:\n",
    "    median_val = X[col].median()\n",
    "    if pd.isna(median_val):\n",
    "        median_val = 0\n",
    "    X[col] = X[col].fillna(median_val)\n",
    "    X_test[col] = X_test[col].fillna(median_val)\n",
    "\n",
    "print(f\"\\n‚úÖ X shape: {X.shape}\")\n",
    "print(f\"‚úÖ X_test shape: {X_test.shape}\")\n",
    "print(f\"‚úÖ TDE count: {y.sum()} ({100*y.mean():.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13: Train Models (LightGBM + XGBoost Ensemble)\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "oof_lgb = np.zeros(len(X))\n",
    "oof_xgb = np.zeros(len(X))\n",
    "test_lgb = np.zeros(len(X_test))\n",
    "test_xgb = np.zeros(len(X_test))\n",
    "\n",
    "# Class weight\n",
    "scale_pos_weight = (y == 0).sum() / (y == 1).sum()\n",
    "print(f\"Scale pos weight: {scale_pos_weight:.1f}\")\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.03,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 20,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1,\n",
    "    'num_iterations': 2000,\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.03,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'n_estimators': 2000,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'verbosity': 0,\n",
    "}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{N_FOLDS} ---\")\n",
    "    \n",
    "    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    print(f\"Train: {len(y_tr)} (TDE: {y_tr.sum()}), Val: {len(y_val)} (TDE: {y_val.sum()})\")\n",
    "    \n",
    "    # LightGBM\n",
    "    lgb_train = lgb.Dataset(X_tr, label=y_tr)\n",
    "    lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n",
    "    lgb_model = lgb.train(lgb_params, lgb_train, valid_sets=[lgb_val],\n",
    "                         callbacks=[lgb.early_stopping(100)])\n",
    "    \n",
    "    oof_lgb[val_idx] = lgb_model.predict(X_val)\n",
    "    test_lgb += lgb_model.predict(X_test) / N_FOLDS\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb_model = xgb.XGBClassifier(**xgb_params, random_state=RANDOM_STATE)\n",
    "    xgb_model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    \n",
    "    oof_xgb[val_idx] = xgb_model.predict_proba(X_val)[:, 1]\n",
    "    test_xgb += xgb_model.predict_proba(X_test)[:, 1] / N_FOLDS\n",
    "    \n",
    "    # Fold F1 at 0.5 threshold\n",
    "    lgb_f1 = f1_score(y_val, (oof_lgb[val_idx] > 0.5).astype(int))\n",
    "    xgb_f1 = f1_score(y_val, (oof_xgb[val_idx] > 0.5).astype(int))\n",
    "    print(f\"LGB F1 @0.5: {lgb_f1:.4f}, XGB F1 @0.5: {xgb_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 14: Ensemble and Threshold Optimization\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENSEMBLE & THRESHOLD OPTIMIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Try different ensemble weights\n",
    "best_f1 = 0\n",
    "best_config = None\n",
    "\n",
    "for lgb_w in [0.5, 0.6, 0.7, 0.8]:\n",
    "    xgb_w = 1 - lgb_w\n",
    "    oof_ens = lgb_w * oof_lgb + xgb_w * oof_xgb\n",
    "    \n",
    "    for t in np.arange(0.05, 0.5, 0.01):\n",
    "        f1 = f1_score(y, (oof_ens > t).astype(int), zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_config = {'lgb_w': lgb_w, 'xgb_w': xgb_w, 'threshold': t}\n",
    "\n",
    "print(f\"\\nüèÜ BEST CONFIG:\")\n",
    "print(f\"   LGB weight: {best_config['lgb_w']}\")\n",
    "print(f\"   XGB weight: {best_config['xgb_w']}\")\n",
    "print(f\"   Threshold: {best_config['threshold']:.3f}\")\n",
    "print(f\"   OOF F1: {best_f1:.4f}\")\n",
    "\n",
    "# Also show individual model performance\n",
    "print(\"\\n--- Individual Model Performance ---\")\n",
    "for name, preds in [('LGB', oof_lgb), ('XGB', oof_xgb)]:\n",
    "    best_t = 0.1\n",
    "    best_f = 0\n",
    "    for t in np.arange(0.05, 0.5, 0.01):\n",
    "        f1 = f1_score(y, (preds > t).astype(int), zero_division=0)\n",
    "        if f1 > best_f:\n",
    "            best_f = f1\n",
    "            best_t = t\n",
    "    print(f\"{name}: F1={best_f:.4f} @ t={best_t:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 15: Feature Importance\n",
    "print(\"=\"*70)\n",
    "print(\"TOP 25 FEATURES (by LightGBM gain)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "importance = lgb_model.feature_importance(importance_type='gain')\n",
    "imp_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(imp_df.head(25).to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(imp_df['feature'].head(20)[::-1], imp_df['importance'].head(20)[::-1])\n",
    "plt.xlabel('Importance (Gain)')\n",
    "plt.title('Top 20 Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 16: Create Submissions\n",
    "print(\"=\"*70)\n",
    "print(\"CREATING SUBMISSIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Best ensemble\n",
    "test_probs = best_config['lgb_w'] * test_lgb + best_config['xgb_w'] * test_xgb\n",
    "best_t = best_config['threshold']\n",
    "\n",
    "predictions = (test_probs > best_t).astype(int)\n",
    "print(f\"\\nBest config predictions:\")\n",
    "print(f\"   Threshold: {best_t:.3f}\")\n",
    "print(f\"   Predicted TDEs: {predictions.sum()} ({100*predictions.mean():.2f}%)\")\n",
    "\n",
    "# Main submission\n",
    "submission = pd.DataFrame({\n",
    "    'object_id': test_df['object_id'],\n",
    "    'prediction': predictions\n",
    "})\n",
    "submission.to_csv('/content/submission_v5_golden.csv', index=False)\n",
    "print(f\"\\n‚úÖ Saved submission_v5_golden.csv\")\n",
    "\n",
    "# Alternative thresholds around best\n",
    "print(\"\\nAlternative submissions:\")\n",
    "for t_offset in [-0.05, -0.02, 0.02, 0.05]:\n",
    "    t = best_t + t_offset\n",
    "    if 0.05 <= t <= 0.5:\n",
    "        preds_t = (test_probs > t).astype(int)\n",
    "        sub_t = pd.DataFrame({'object_id': test_df['object_id'], 'prediction': preds_t})\n",
    "        fname = f'/content/submission_v5_t{int(t*100):02d}.csv'\n",
    "        sub_t.to_csv(fname, index=False)\n",
    "        print(f\"  t={t:.2f}: {preds_t.sum()} TDEs ({100*preds_t.mean():.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 17: Download\n",
    "from google.colab import files\n",
    "print(\"üì• Downloading main submission...\")\n",
    "files.download('/content/submission_v5_golden.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 18: Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üåü V5 GOLDEN FEATURES SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "Strategy: Physics-based feature engineering, NO augmentation\n",
    "\n",
    "Golden Features Added:\n",
    "1. SHAPE: rise_time, decay_time, asymmetry, FWHM, t10-t90\n",
    "2. POWER-LAW: decay_power_index, tde_index_match (vs t^-5/3)\n",
    "3. COLOR: u_r ratios, g_r_at_peak, color evolution\n",
    "4. STOCHASTICITY: flux_scatter, chi2_smooth, n_local_peaks\n",
    "5. BAZIN FIT: t_fall, t_rise, rise_fall_ratio\n",
    "\n",
    "Results:\n",
    "- OOF F1: {best_f1:.4f}\n",
    "- Optimal threshold: {best_config['threshold']:.3f}\n",
    "- LGB weight: {best_config['lgb_w']}\n",
    "- Predicted TDEs: {predictions.sum()}\n",
    "\n",
    "Comparison:\n",
    "- V2 (baseline): 0.63\n",
    "- V3 (50x aug): 0.40 ‚ùå\n",
    "- V4 (3x aug): 0.618\n",
    "- V5 (golden): {best_f1:.4f}\n",
    "\n",
    "Top discriminative features (check importance plot):\n",
    "- Look for color features (u_r, g_r) ‚Üí TDE is blue\n",
    "- Look for asymmetry features ‚Üí TDE has long decay\n",
    "- Look for stochasticity features ‚Üí TDE is smooth\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
